{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤描述得还不错\n",
    "\n",
    "https://blog.csdn.net/u012223913/article/details/75051516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-103d78ea1a9d>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "sess = tf.InteractiveSession()\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 声明变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_var(shape, name):\n",
    "    return tf.get_variable(name=name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "def bias_var(shape, name):\n",
    "    return tf.get_variable(name=name, shape=shape, initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "\n",
    "# discriminater net\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784], name='X')\n",
    "\n",
    "D_W1 = weight_var([784, 128], 'D_W1')\n",
    "D_b1 = bias_var([128], 'D_b1')\n",
    "\n",
    "D_W2 = weight_var([128, 1], 'D_W2')\n",
    "D_b2 = bias_var([1], 'D_b2')\n",
    "\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "\n",
    "# generator net\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100], name='Z')\n",
    "\n",
    "G_W1 = weight_var([100, 128], 'G_W1')\n",
    "G_b1 = bias_var([128], 'G_B1')\n",
    "\n",
    "G_W2 = weight_var([128, 784], 'G_W2')\n",
    "G_b2 = bias_var([784], 'G_B2')\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    #G_prob = tf.nn.tanh(G_log_prob)\n",
    "    #G_prob = tf.nn.relu(G_log_prob)\n",
    "\n",
    "    return G_prob\n",
    "\n",
    "\n",
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    #D_prob = tf.nn.tanh(D_logit)\n",
    "    #D_prob = tf.nn.relu(D_logit)\n",
    "    return D_prob, D_logit\n",
    "\n",
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设定loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于tensorflow只能做minimize，loss function可以写成如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，论文中提到，比起最小化 tf.reduce_mean(1 - tf.log(D_fake)) ，最大化tf.reduce_mean(tf.log(D_fake)) 更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_optimizer = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_optimizer = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "D loss: 0.004493\n",
      "G_loss: 6.26\n",
      "\n",
      "Iter: 1\n",
      "D loss: 0.002525\n",
      "G_loss: 6.928\n",
      "\n",
      "Iter: 2\n",
      "D loss: 0.004488\n",
      "G_loss: 7.613\n",
      "\n",
      "Iter: 3\n",
      "D loss: 0.0005325\n",
      "G_loss: 8.749\n",
      "\n",
      "Iter: 4\n",
      "D loss: 0.001058\n",
      "G_loss: 7.282\n",
      "\n",
      "Iter: 5\n",
      "D loss: 0.003373\n",
      "G_loss: 8.076\n",
      "\n",
      "Iter: 6\n",
      "D loss: 0.0001259\n",
      "G_loss: 10.69\n",
      "\n",
      "Iter: 7\n",
      "D loss: 0.01119\n",
      "G_loss: 10.62\n",
      "\n",
      "Iter: 8\n",
      "D loss: 0.01177\n",
      "G_loss: 9.979\n",
      "\n",
      "Iter: 9\n",
      "D loss: 0.04229\n",
      "G_loss: 6.829\n",
      "\n",
      "Iter: 10\n",
      "D loss: 0.05948\n",
      "G_loss: 6.529\n",
      "\n",
      "Iter: 11\n",
      "D loss: 0.005389\n",
      "G_loss: 6.192\n",
      "\n",
      "Iter: 12\n",
      "D loss: 0.02695\n",
      "G_loss: 5.841\n",
      "\n",
      "Iter: 13\n",
      "D loss: 0.003235\n",
      "G_loss: 8.013\n",
      "\n",
      "Iter: 14\n",
      "D loss: 0.01693\n",
      "G_loss: 8.108\n",
      "\n",
      "Iter: 15\n",
      "D loss: 0.006265\n",
      "G_loss: 7.456\n",
      "\n",
      "Iter: 16\n",
      "D loss: 0.0652\n",
      "G_loss: 7.917\n",
      "\n",
      "Iter: 17\n",
      "D loss: 0.01318\n",
      "G_loss: 8.199\n",
      "\n",
      "Iter: 18\n",
      "D loss: 0.02317\n",
      "G_loss: 6.053\n",
      "\n",
      "Iter: 19\n",
      "D loss: 0.008585\n",
      "G_loss: 7.075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mb_size = 128\n",
    "Z_dim = 100\n",
    "\n",
    "# sample_Z均匀初始化\n",
    "# fan_in是输入节点的数量，fan_out是输出节点的数量。\n",
    "# def sample_Z(fan_in, fan_out, constant=1):\n",
    "#     low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "#     high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "#     # 通过tf.random_uniform创建了一个均匀分布\n",
    "#     return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def sample_Z(m, n):\n",
    "    '''Uniform prior for G(Z)'''\n",
    "    fan_in = m\n",
    "    fan_out = n\n",
    "    low = -np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return np.random.uniform(low, high, size=[m, n])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "n_samples = int(mnist.train.num_examples)#训练样本的总数\n",
    "training_epochs = 20 #最大训练轮数，每n_samples/batch_size个批次为1轮\n",
    "\n",
    "for it in range(training_epochs):\n",
    "    total_batch = int(n_samples / mb_size)\n",
    "    for i in range(total_batch):\n",
    "        X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "        # Z参数用于G_sample = generator(Z) 生成fake图像\n",
    "        _, D_loss_curr = sess.run([D_optimizer, D_loss], feed_dict={\n",
    "                                  X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "        _, G_loss_curr = sess.run([G_optimizer, G_loss], feed_dict={\n",
    "                                  Z: sample_Z(mb_size, Z_dim)})\n",
    "    \n",
    "    print('Iter: {}'.format(it))\n",
    "    print('D loss: {:.4}'.format(D_loss_curr))\n",
    "    print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存生成的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):  # [i,samples[i]] imax=16\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "i = 0\n",
    "for it in range(1000000):\n",
    "    if it % 1000 == 0:\n",
    "        samples = sess.run(G_sample, feed_dict={\n",
    "                           Z: sample_Z(16, Z_dim)})  # 16*784\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "\n",
    "    _, D_loss_curr = sess.run([D_optimizer, D_loss], feed_dict={\n",
    "                              X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "    _, G_loss_curr = sess.run([G_optimizer, G_loss], feed_dict={\n",
    "                              Z: sample_Z(mb_size, Z_dim)})\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'.format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用到了卷积网络\n",
    "\n",
    "https://blog.csdn.net/weixin_42234472/article/details/88876471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')\n",
    "img = mnist.train.images[50]\n",
    " \n",
    "def get_inputs(real_size, noise_size):\n",
    "    \"\"\"\n",
    "    真实图像tensor与噪声图像tensor\n",
    "    \"\"\"\n",
    "    real_img = tf.placeholder(tf.float32, [None, real_size], name='real_img')\n",
    "    noise_img = tf.placeholder(tf.float32, [None, noise_size], name='noise_img')    \n",
    "    return real_img, noise_img\n",
    "\n",
    "def get_generator(noise_img, n_units, out_dim, reuse=False, alpha=0.01):\n",
    "    \"\"\"\n",
    "    生成器    \n",
    "    noise_img: 生成器的输入\n",
    "    n_units: 隐层单元个数\n",
    "    out_dim: 生成器输出tensor的size，这里应该为32*32=784\n",
    "    alpha: leaky ReLU系数\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "        # hidden layer\n",
    "        hidden1 = tf.layers.dense(noise_img, n_units)\n",
    "        # leaky ReLU\n",
    "        hidden1 = tf.maximum(alpha * hidden1, hidden1)\n",
    "        # dropout\n",
    "        hidden1 = tf.layers.dropout(hidden1, rate=0.2)\n",
    "        # logits & outputs\n",
    "        logits = tf.layers.dense(hidden1, out_dim)\n",
    "        outputs = tf.tanh(logits)     \n",
    "        return logits, outputs\n",
    "\n",
    "def get_discriminator(img, n_units, reuse=False, alpha=0.01):\n",
    "    \"\"\"\n",
    "    判别器\n",
    "    \n",
    "    n_units: 隐层结点数量\n",
    "    alpha: Leaky ReLU系数\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        # hidden layer\n",
    "        hidden1 = tf.layers.dense(img, n_units)\n",
    "        hidden1 = tf.maximum(alpha * hidden1, hidden1)\n",
    "        \n",
    "        # logits & outputs\n",
    "        logits = tf.layers.dense(hidden1, 1)\n",
    "        outputs = tf.sigmoid(logits)\n",
    "        \n",
    "        return logits, outputs\n",
    "    \n",
    "# 定义参数\n",
    "# 真实图像的size\n",
    "img_size = mnist.train.images[0].shape[0]\n",
    "# 传入给generator的噪声size\n",
    "noise_size = 100\n",
    "# 生成器隐层参数\n",
    "g_units = 128\n",
    "# 判别器隐层参数\n",
    "d_units = 128\n",
    "# leaky ReLU的参数\n",
    "alpha = 0.01\n",
    "# learning_rate\n",
    "learning_rate = 0.001\n",
    "# label smoothing\n",
    "smooth = 0.1\n",
    " \n",
    "tf.reset_default_graph()\n",
    " \n",
    "real_img, noise_img = get_inputs(img_size, noise_size)\n",
    " \n",
    "# generator\n",
    "g_logits, g_outputs = get_generator(noise_img, g_units, img_size)\n",
    " \n",
    "# discriminator\n",
    "d_logits_real, d_outputs_real = get_discriminator(real_img, d_units)\n",
    "d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, d_units, reuse=True)\n",
    "# discriminator的loss\n",
    "# 识别真实图片\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, \n",
    "                                                                     labels=tf.ones_like(d_logits_real)) * (1 - smooth))\n",
    "# 识别生成的图片\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n",
    "                                                                     labels=tf.zeros_like(d_logits_fake)))\n",
    "# 总体loss\n",
    "d_loss = tf.add(d_loss_real, d_loss_fake)\n",
    " \n",
    "# generator的loss\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                labels=tf.ones_like(d_logits_fake)) * (1 - smooth))\n",
    " \n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    " \n",
    "# generator中的tensor\n",
    "g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n",
    "# discriminator中的tensor\n",
    "d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n",
    " \n",
    "# optimizer\n",
    "d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    " \n",
    "\n",
    "#训练\n",
    "# batch_size\n",
    "batch_size = 64\n",
    "# 训练迭代轮数\n",
    "epochs = 300\n",
    "# 抽取样本数\n",
    "n_sample = 25\n",
    " \n",
    "# 存储测试样例\n",
    "samples = []\n",
    "# 存储loss\n",
    "losses = []\n",
    "# 保存生成器变量\n",
    "saver = tf.train.Saver(var_list = g_vars)\n",
    "# 开始训练\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for batch_i in range(mnist.train.num_examples//batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            # 对图像像素进行scale，这是因为tanh输出的结果介于(-1,1),real和fake图片共享discriminator的参数\n",
    "            batch_images = batch_images*2 - 1\n",
    "            \n",
    "            # generator的输入噪声\n",
    "            batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size))\n",
    "            \n",
    "            # Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={real_img: batch_images, noise_img: batch_noise})\n",
    "            _ = sess.run(g_train_opt, feed_dict={noise_img: batch_noise})\n",
    "        \n",
    "        # 每一轮结束计算loss\n",
    "        train_loss_d = sess.run(d_loss, \n",
    "                                feed_dict = {real_img: batch_images, \n",
    "                                             noise_img: batch_noise})\n",
    "        # real img loss\n",
    "        train_loss_d_real = sess.run(d_loss_real, \n",
    "                                     feed_dict = {real_img: batch_images, \n",
    "                                                 noise_img: batch_noise})\n",
    "        # fake img loss\n",
    "        train_loss_d_fake = sess.run(d_loss_fake, \n",
    "                                    feed_dict = {real_img: batch_images, \n",
    "                                                 noise_img: batch_noise})\n",
    "        # generator loss\n",
    "        train_loss_g = sess.run(g_loss, \n",
    "                                feed_dict = {noise_img: batch_noise})\n",
    "        \n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}(Real: {:.4f} + Fake: {:.4f})...\".format(train_loss_d, train_loss_d_real, train_loss_d_fake),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g))    \n",
    "        # 记录各类loss值\n",
    "        losses.append((train_loss_d, train_loss_d_real, train_loss_d_fake, train_loss_g))\n",
    "        \n",
    "        # 抽取样本后期进行观察\n",
    "        sample_noise = np.random.uniform(-1, 1, size=(n_sample, noise_size))\n",
    "        gen_samples = sess.run(get_generator(noise_img, g_units, img_size, reuse=True),\n",
    "                               feed_dict={noise_img: sample_noise})\n",
    "        samples.append(gen_samples)\n",
    "        \n",
    "        # 存储checkpoints\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "\n",
    "# 将sample的生成数据记录下来\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pickle.dump(samples, f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#    绘制loss曲线   \n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator Total Loss')\n",
    "plt.plot(losses.T[1], label='Discriminator Real Loss')\n",
    "plt.plot(losses.T[2], label='Discriminator Fake Loss')\n",
    "plt.plot(losses.T[3], label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pickle.load(f)\n",
    "\n",
    "# 指定要查看的轮次\n",
    "epoch_idx = [0, 5, 10, 20, 40, 60, 80, 100, 150, 250] # 一共300轮，不要越界\n",
    "show_imgs = []\n",
    "for i in epoch_idx:\n",
    "    show_imgs.append(samples[i][1])\n",
    "\n",
    "# 指定图片形状\n",
    "rows, cols = 10, 25\n",
    "fig, axes = plt.subplots(figsize=(30,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
    " \n",
    "idx = range(0, epochs, int(epochs/rows))\n",
    " \n",
    "for sample, ax_row in zip(show_imgs, axes):\n",
    "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n",
    "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        \n",
    "# --------------------- \n",
    "# 作者：Rui012345 \n",
    "# 来源：CSDN \n",
    "# 原文：https://blog.csdn.net/qq_27855219/article/details/83749756 \n",
    "# 版权声明：本文为博主原创文章，转载请附上博文链接！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tensorflow不同层的使用](https://blog.csdn.net/holmes_MX/article/details/82317742)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
