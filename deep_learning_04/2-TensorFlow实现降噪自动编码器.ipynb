{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生AdditiveGaussianNoiseAutoencoder类对象实例\n",
      "begin ti run session...\n",
      "把计算图写入事件文件，在TensorBoard里面查看\n",
      "WARNING:tensorflow:From <ipython-input-1-d09b874fa348>:125: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "epoch : 0001,cost = 214324.026683354\n",
      "epoch : 0002,cost = 452579.160130321\n",
      "epoch : 0003,cost = 2429781.400312659\n",
      "epoch : 0004,cost = 23770.203580487\n",
      "epoch : 0005,cost = 26094.066114448\n",
      "epoch : 0006,cost = 51185.423942441\n",
      "epoch : 0007,cost = 40672.130181871\n",
      "epoch : 0008,cost = 4822.769255667\n",
      "epoch : 0009,cost = 102984.617945424\n",
      "epoch : 0010,cost = 2233579.197434716\n",
      "epoch : 0011,cost = 37468.114559858\n",
      "epoch : 0012,cost = 1870604.368216988\n",
      "epoch : 0013,cost = 175389.517783425\n",
      "epoch : 0014,cost = 34358.223729131\n",
      "epoch : 0015,cost = 50541.830534181\n",
      "epoch : 0016,cost = 5771.056355652\n",
      "epoch : 0017,cost = 10982.105965710\n",
      "epoch : 0018,cost = 153596.364318492\n",
      "epoch : 0019,cost = 718.213933131\n",
      "epoch : 0020,cost = 22955.139456689\n",
      "Total cost: 33930452.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Xvaier均匀初始化\n",
    "# fan_in是输入节点的数量，fan_out是输出节点的数量。\n",
    "def xavier_init(fan_in, fan_out, constant=1):\n",
    "    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high, dtype=tf.float32)\n",
    "\n",
    "# 加性高斯噪声的自动编码\n",
    "class AdditiveGaussianNoiseAutoencoder(object):\n",
    "    # 在初始的数据中加入高斯噪声。在实现降噪自编码器的时候，\n",
    "    # 只是在输入加进去的时候，在输入上加上高斯噪声就行\n",
    "    # 其他的部分和基本自编码器一样\n",
    "    # n_input：输入变量数；n_hidden：隐含层节点数；transfer_function：隐含层激活函数；optimizer：优化器；scale：高斯噪声系数；\n",
    "    # Class内的scale声明成一个占位符placeholder，参数初始化则使用了接下来定义的_initialize_weights函数。我们只用了一个隐含层。\n",
    "    def __init__(self, n_input, n_hidden, transfer_function=tf.nn.softplus, optimizer=tf.train.AdamOptimizer(),scale=0.1):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        # n_input,n_hidden都是输入和隐藏层的维度\n",
    "        self.transfer = transfer_function\n",
    "        # self.scale = tf.placeholder(tf.float32)\n",
    "        self.training_scale = scale\n",
    "        # scale 就是一个标量\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "\n",
    "        # model\n",
    "        with tf.name_scope(\"RawInput\"):\n",
    "            self.x = tf.placeholder(tf.float32, [None, self.n_input])\n",
    "        # none不给定具体的值，它由输入的数目来决定\n",
    "\n",
    "        with tf.name_scope(\"NoiseAdder\"):\n",
    "            self.scale = tf.placeholder(tf.float32)\n",
    "            self.noise_x = self.x + self.scale * tf.random_normal((n_input,))\n",
    "\n",
    "        with tf.name_scope(\"Encoder\"):\n",
    "            self.hidden = self.transfer(tf.add(tf.matmul(self.noise_x,self.weights['w1']), self.weights['b1']))\n",
    "        # 在输入的时候，在输入的数据上加上一些高斯噪声，\n",
    "        # tf.random_normal((n_input,)) 默认给的是一个均值为0，标准差是1的正态分布的随机数。\n",
    "\n",
    "        with tf.name_scope(\"Reconstruction\"):\n",
    "            self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights['w2']), self.weights['b2'])\n",
    "\n",
    "        # x:一维的数量为n_input的placeholder；\n",
    "        # 建立一个能提取特征的隐含层：\n",
    "        # 先对输入x添加高斯噪声，即self.x + scale * tf.random_normal((n_input,))；\n",
    "        #  再用tf.matmul()让被噪声污染的信号与隐藏层的权重相乘，再用tf.add()添加偏置；\n",
    "        # 最后使用transfer()对加权汇总结果进行激活函数处理。\n",
    "\n",
    "        # 经过隐藏层后，在输出层进行数据复原和重建操作，即建立reconstruction层，这时候就不需要激活函数了，\n",
    "        # 直接将隐含层的输出self.hidden乘以输出层的权重w2再加上输出层的偏置b2。\n",
    "\n",
    "        # cost\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))\n",
    "\n",
    "        with tf.name_scope(\"Train\"):\n",
    "            self.optimizer = optimizer.minimize(self.cost)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        print(\"begin ti run session...\")\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        all_weights['w1'] = tf.Variable(xavier_init(self.n_input, self.n_hidden),name='weight1')\n",
    "        # 在初始化的时候，就只是第一个权重矩阵是赋予一些随机值，其他的都是赋予全0矩阵\n",
    "        all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32),name='bias1')\n",
    "        all_weights['w2'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype=tf.float32),name='weight2')\n",
    "        all_weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32),name='bias2')\n",
    "        return all_weights\n",
    "    #变量初始化函数：创建一个dicts，包含所有Variable类型的变量w1，b1，w2，b2\n",
    "    #隐藏层：w1用前面写的xavier_init函数初始化，传入输入节点数和隐含节点数，即可得到一个比较适合softplus等激活函数的初始状态，b1初始化为0\n",
    "\n",
    "    #在一个批次上训练模型\n",
    "    def partial_fit(self,X):\n",
    "        cost,opt = self.sess.run((self.cost,self.optimizer),feed_dict={self.x:X,self.scale:self.training_scale})\n",
    "        return cost\n",
    "\n",
    "    #在给定的样本集合上计算损失（用于测试阶段）\n",
    "    def calc_total_cost(self,X):\n",
    "        return self.sess.run(self.cost,feed_dict={self.x:X,self.scale:self.training_scale})\n",
    "\n",
    "    #返回自编码器隐含层的输出结果，获得抽象后的高阶特征表示\n",
    "    def transform(self,X):\n",
    "        return self.sess.run(self.hidden,feed_dict={self.x:X,self.scale:self.training_scale})\n",
    "\n",
    "    #将隐藏层的高阶特征作为输入，将其重建为原始输入数据\n",
    "    def generate(self,hidden=None):\n",
    "        if hidden == None:\n",
    "            hidden = np.random.normal(size=self.weights['b1'])\n",
    "        return self.sess.run(self.reconstruction,feed_dict={self.hidden:hidden})\n",
    "\n",
    "\n",
    "    #整体运行一遍复原过程，包括提取高阶特征以及重建原始数据，输入原始数据，输出复原后的数据\n",
    "    def reconstruction(self,X):\n",
    "        return self.sess.run(self.reconstruction,feed_dict={self.x:X,self.scale:self.training_scale})\n",
    "\n",
    "    #获取隐含层的权重\n",
    "    def getWeights(self):\n",
    "        return self.sess.run(self.weights['w1'])\n",
    "\n",
    "    #获取隐含层的偏置\n",
    "    def getBiases(self):\n",
    "        return self.sess.run(self.weights['b1'])\n",
    "\n",
    "print('产生AdditiveGaussianNoiseAutoencoder类对象实例')\n",
    "#产生一个AdditiveGaussianNoiseAutoencoder类的对象实例，调用tf.summary.FileWriter把计算图写入文件，使用TensorBoard查看\n",
    "AGN_AutoEncoder = AdditiveGaussianNoiseAutoencoder(n_input=784,n_hidden=200,transfer_function=tf.nn.softplus,\n",
    "                                          optimizer=tf.train.AdamOptimizer(learning_rate=0.01),scale=0.01)\n",
    "\n",
    "print('把计算图写入事件文件，在TensorBoard里面查看')\n",
    "writer = tf.summary.FileWriter(logdir='logs',graph=AGN_AutoEncoder.sess.graph)\n",
    "writer.close()\n",
    "\n",
    "#读取数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)\n",
    "# 考虑多类情况。非onehot，标签是类似0  1  2   3...n这样。而onehot标签则是顾名思义，一个长度为n的数组，只有一个元素是1.0，其他元素是0.0。\n",
    "# 例如在n为4的情况下，标签2对应的onehot标签就是 0.0  0.0   1.0  0.0使用onehot的直接原因是现在多分类cnn网络的输出通常是softmax层，\n",
    "# 而它的输出是一个概率分布，从而要求输入的标签也以概率分布的形式出现，进而算交叉熵之类。\n",
    "\n",
    "#使用sklearn.preprocess的数据标准化操作（0均值标准差为1)预处理数据\n",
    "#首先在训练集上估计均值与方差，然后将其作用到训练集和测试集\n",
    "#StandardScaler:尺度标准化\n",
    "def standard_scale(X_train,X_test):\n",
    "    preprocessor = prep.StandardScaler().fit(X_train)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    return X_train,X_test\n",
    "#获取随机block数据的函数：取一个0到len（data） -  batch_size的随机整数\n",
    "#以这个随机整数为起始索引，抽出一个batch_size的批次的样本\n",
    "\n",
    "def get_random_block_from_data(data,batch_size):\n",
    "    start_index = np.random.randint(0,len(data) - batch_size)\n",
    "    return  data[start_index:(start_index + batch_size)]\n",
    "\n",
    "#使用标准化操作变换数据集，\n",
    "X_train,X_test = standard_scale(mnist.train.images,mnist.test.images)\n",
    "\n",
    "#定义训练参数\n",
    "n_samples = int(mnist.train.num_examples)#训练样本的总数\n",
    "training_epochs = 20 #最大训练轮数，每n_samples/batch_size个批次为1轮\n",
    "batch_size = 128     #每个批次的样本数量\n",
    "display_step = 1     #输出训练结果的间隔\n",
    "\n",
    "#下面开始训练过程，在每一轮epoch训练开始时，将平均损失avg_cost设为0\n",
    "#计算总共需要的batch数量（样本总数/batch_size），这里使用的是有放回抽样，\n",
    "#所以不能保证每个样本被抽到并参与训练\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    #在每个batch的循环中，随机抽取一个batch的数据，使用成员函数partial_fit训练这个batch的数据，\n",
    "    #计算cost，累积到当前回合的平均cost中\n",
    "    for i in range(total_batch):\n",
    "        batch_xs = get_random_block_from_data(X_train,batch_size)\n",
    "        cost = AGN_AutoEncoder.partial_fit(batch_xs)\n",
    "        avg_cost += cost / batch_size\n",
    "    avg_cost /= total_batch\n",
    "\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"epoch : %04d,cost = %.9f\" % (epoch+1,avg_cost))\n",
    "\n",
    "print(\"Total cost:\",str(AGN_AutoEncoder.calc_total_cost(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
