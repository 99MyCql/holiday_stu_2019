{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把计算图写入事件文件，在TensorBoard里面查看\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# 设置算法超参数\n",
    "learning_rate_init = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 32\n",
    "display_step = 10\n",
    "conv1_kernel_num = 64\n",
    "conv2_kernel_num = 192\n",
    "conv3_kernel_num = 384\n",
    "conv4_kernel_num = 256\n",
    "conv5_kernel_num = 256\n",
    "\n",
    "#数据集输入图像的参数\n",
    "image_size = 224\n",
    "image_channel = 3\n",
    "n_classes = 1000\n",
    "\n",
    "fc1_units_num = 4096\n",
    "fc2_units_num = 4096\n",
    "activation_func = tf.nn.relu\n",
    "activation_name = 'relu'\n",
    "\n",
    "#根据指定的维数返回初始化好的指定名称的权重 Variable\n",
    "def WeightsVariable(shape, name_str, stddev=0.1):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=stddev, dtype=tf.float32)\n",
    "    return tf.Variable(initial, dtype=tf.float32, name=name_str)\n",
    "\n",
    "#根据指定的维数返回初始化好的指定名称的偏置 Variable\n",
    "def BiasesVariable(shape, name_str, init_value=0.00001):\n",
    "    initial = tf.constant(init_value, shape=shape)\n",
    "    return tf.Variable(initial, dtype=tf.float32, name=name_str)\n",
    "\n",
    "# 二维卷积层activation(conv2d+bias)的封装\n",
    "def Conv2d(x, W, b, stride=1, padding='SAME',activation=tf.nn.relu,act_name='relu'):\n",
    "    with tf.name_scope('conv2d_bias'):\n",
    "        y = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=padding)\n",
    "        y = tf.nn.bias_add(y, b)\n",
    "    with tf.name_scope(act_name):\n",
    "        y = activation(y)\n",
    "    return y\n",
    "\n",
    "# 二维池化层pool的封装\n",
    "def Pool2d(x, pool= tf.nn.max_pool, k=2, stride=2,padding='SAME'):\n",
    "    return pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding=padding)\n",
    "\n",
    "# 全连接层activate(wx+b)的封装\n",
    "def FullyConnected(x, W, b, activation=tf.nn.relu, act_name='relu'):\n",
    "    with tf.name_scope('Wx_b'):\n",
    "        y = tf.matmul(x, W)\n",
    "        y = tf.add(y, b)\n",
    "    with tf.name_scope(act_name):\n",
    "        y = activation(y)\n",
    "    return y\n",
    "\n",
    "#修改了4处激活函数：Conv2d_1、Conv2d_2、FC1_nonlinear、FC2_nonlinear\n",
    "def Inference(image_holder):\n",
    "    # 第一个卷积层activate(conv2d + biase)\n",
    "    with tf.name_scope('Conv2d_1'):\n",
    "        weights = WeightsVariable(shape=[11, 11, image_channel, conv1_kernel_num],name_str='weights',stddev=1e-1)\n",
    "        biases = BiasesVariable(shape=[conv1_kernel_num], name_str='biases',init_value=0.0)\n",
    "        conv1_out = Conv2d(image_holder, weights, biases, stride=4, padding='SAME')#输出：[32x56x56x64]\n",
    "\n",
    "    # 第一个池化层(pool 2d)\n",
    "    with tf.name_scope('Pool2d_1'):\n",
    "        pool1_out = Pool2d(conv1_out, pool=tf.nn.max_pool, k=3, stride=2,padding='VALID')\n",
    "\n",
    "    # 第二个卷积层activate(conv2d + biase)\n",
    "    with tf.name_scope('Conv2d_2'):\n",
    "        weights = WeightsVariable(shape=[5, 5, conv1_kernel_num, conv2_kernel_num], name_str='weights', stddev=1e-1)\n",
    "        biases = BiasesVariable(shape=[conv2_kernel_num], name_str='biases', init_value=0.0)\n",
    "        conv2_out = Conv2d(pool1_out, weights, biases, stride=1, padding='SAME')\n",
    "\n",
    "    # 第二个池化层(pool 2d)\n",
    "    with tf.name_scope('Pool2d_2'):\n",
    "        pool2_out = Pool2d(conv2_out, pool=tf.nn.max_pool, k=3, stride=2, padding='VALID')\n",
    "\n",
    "    # 第三个卷积层activate(conv2d + biase)\n",
    "    with tf.name_scope('Conv2d_3'):\n",
    "        weights = WeightsVariable(shape=[3, 3, conv2_kernel_num, conv3_kernel_num], name_str='weights', stddev=1e-1)\n",
    "        biases = BiasesVariable(shape=[conv3_kernel_num], name_str='biases', init_value=0.0)\n",
    "        conv3_out = Conv2d(pool2_out, weights, biases, stride=1, padding='SAME')\n",
    "\n",
    "    # 第四个卷积层activate(conv2d + biase)\n",
    "    with tf.name_scope('Conv2d_4'):\n",
    "        weights = WeightsVariable(shape=[3, 3, conv3_kernel_num, conv4_kernel_num], name_str='weights', stddev=1e-1)\n",
    "        biases = BiasesVariable(shape=[conv4_kernel_num], name_str='biases', init_value=0.0)\n",
    "        conv4_out = Conv2d(conv3_out, weights, biases, stride=1, padding='SAME')\n",
    "\n",
    "    # 第五个卷积层activate(conv2d + biase)\n",
    "    with tf.name_scope('Conv2d_5'):\n",
    "        weights = WeightsVariable(shape=[3, 3, conv4_kernel_num, conv5_kernel_num], name_str='weights', stddev=1e-1)\n",
    "        biases = BiasesVariable(shape=[conv5_kernel_num], name_str='biases', init_value=0.0)\n",
    "        conv5_out = Conv2d(conv4_out, weights, biases, stride=1, padding='SAME')\n",
    "\n",
    "    # 第五个池化层(pool 2d)\n",
    "    with tf.name_scope('Pool2d_5'):\n",
    "        pool5_out = Pool2d(conv5_out, pool=tf.nn.max_pool, k=3, stride=2, padding='VALID')\n",
    "\n",
    "    #将二维特征图变换为一维特征向量\n",
    "    with tf.name_scope('FeatsReshape'):\n",
    "        features = tf.reshape(pool5_out, [batch_size,-1])\n",
    "        feats_dim = features.get_shape()[1].value\n",
    "\n",
    "    # 第一个全连接层(fully connected layer)\n",
    "    with tf.name_scope('FC1_nonlinear'):\n",
    "        weights = WeightsVariable(shape=[feats_dim, fc1_units_num],name_str='weights',stddev=4e-2)\n",
    "        biases = BiasesVariable(shape=[fc1_units_num], name_str='biases',init_value=0.1)\n",
    "        fc1_out = FullyConnected(features, weights, biases, activation=activation_func,act_name=activation_name)\n",
    "\n",
    "    # 第二个全连接层(fully connected layer)\n",
    "    with tf.name_scope('FC2_nonlinear'):\n",
    "        weights = WeightsVariable(shape=[fc1_units_num, fc2_units_num],name_str='weights',stddev=4e-2)\n",
    "        biases = BiasesVariable(shape=[fc2_units_num], name_str='biases',init_value=0.1)\n",
    "        fc2_out = FullyConnected(fc1_out, weights, biases, activation=activation_func,act_name=activation_name)\n",
    "\n",
    "    # 第三个全连接层(fully connected layer)\n",
    "    with tf.name_scope('FC3_linear'):\n",
    "        fc3_units_num = n_classes\n",
    "        weights = WeightsVariable(shape=[fc2_units_num, fc3_units_num],name_str='weights',stddev=1.0/fc2_units_num)\n",
    "        biases = BiasesVariable(shape=[fc3_units_num], name_str='biases',init_value=0.0)\n",
    "        logits = FullyConnected(fc2_out, weights, biases,activation=tf.identity, act_name='linear')\n",
    "    return logits\n",
    "\n",
    "#调用上面写的函数构造计算图\n",
    "with tf.Graph().as_default():\n",
    "    # 计算图输入\n",
    "    with tf.name_scope('Inputs'):\n",
    "        #[32x224x224x3]\n",
    "        image_holder = tf.placeholder(tf.float32, [batch_size, image_size, image_size, image_channel], name='images')\n",
    "        labels_holder = tf.placeholder(tf.int32, [batch_size], name='labels')\n",
    "\n",
    "    # 计算图前向推断过程\n",
    "    with tf.name_scope('Inference'):\n",
    "         logits = Inference(image_holder)\n",
    "\n",
    "    # 定义损失层(loss layer)\n",
    "    with tf.name_scope('Loss'):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_holder,logits=logits)\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "        total_loss_op = cross_entropy_mean\n",
    "\n",
    "    # 定义优化训练层(train layer)\n",
    "    with tf.name_scope('Train'):\n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int64)\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(total_loss_op,global_step=global_step)\n",
    "\n",
    "    # 定义模型评估层(evaluate layer)\n",
    "    with tf.name_scope('Evaluate'):\n",
    "        top_K_op = tf.nn.in_top_k(predictions=logits,targets=labels_holder,k=1)\n",
    "\n",
    "    # 添加所有变量的初始化节点\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    print('把计算图写入事件文件，在TensorBoard里面查看')\n",
    "    summary_writer = tf.summary.FileWriter(logdir='logs/alexnet_graph')\n",
    "    summary_writer.add_graph(graph=tf.get_default_graph())\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
