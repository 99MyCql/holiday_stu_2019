{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一.基本文本处理技能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.词、字符频率统计\n",
    "### 3.1词频率统计：第一步分词，然后根据分词后的结果进行词频率统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今天',\n",
       " '是',\n",
       " '我',\n",
       " '学习',\n",
       " '自然语言',\n",
       " '处理',\n",
       " 'NLP',\n",
       " '的',\n",
       " '计划',\n",
       " '二',\n",
       " '，',\n",
       " '所',\n",
       " '需',\n",
       " '时间',\n",
       " '为',\n",
       " '两天']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = list(jieba.cut('今天是我学习自然语言处理NLP的计划二，所需时间为两天',cut_all=False)) \n",
    "seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'今天': 1, '是': 1, '我': 1, '学习': 1, '自然语言': 1, '处理': 1, 'NLP': 1, '的': 1, '计划': 1, '二': 1, '，': 1, '所': 1, '需': 1, '时间': 1, '为': 1, '两天': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "WordCount = Counter(seg_list)\n",
    "print(WordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter（计数器）：用于追踪值的出现次数\n",
    "\n",
    "Counter类继承dict类，所以它能使用dict类里面的方法 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'c': 3, 'a': 2, 'b': 2})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "obj = collections.Counter('aabbccc')\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[python3 Counter类（计数器）](https://www.cnblogs.com/zhenwei66/p/6593395.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2字符频率统计：按单个字符切分并统计出现频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于collections库的实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'天': 2, '今': 1, '是': 1, '我': 1, '学': 1, '习': 1, '自': 1, '然': 1, '语': 1, '言': 1, '处': 1, '理': 1, 'N': 1, 'L': 1, 'P': 1, '的': 1, '计': 1, '划': 1, '二': 1, '，': 1, '所': 1, '需': 1, '时': 1, '间': 1, '为': 1, '两': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "txt = '今天是我学习自然语言处理NLP的计划二，所需时间为两天'\n",
    "StatisticalCharacters = Counter(txt)\n",
    "print(StatisticalCharacters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.基于的jieba的分词及去停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[简明 jieba 中文分词教程](https://www.jianshu.com/p/883c2171cdb5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 jieba\n",
    "import jieba\n",
    "import jieba.posseg as pseg #词性标注\n",
    "import jieba.analyse as anls #关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词\n",
    "\n",
    "  可使用 jieba.cut 和 jieba.cut_for_search 方法进行分词，两者所返回的结构都是一个可迭代的 generator，可使用 for 循环来获得分词后得到的每一个词语（unicode），或者直接使用 jieba.lcut 以及 jieba.lcut_for_search 直接返回 list。其中：\n",
    "\n",
    "\n",
    "jieba.cut 和 jieba.lcut 接受 3 个参数：\n",
    "\n",
    "- 需要分词的字符串（unicode 或 UTF-8 字符串、GBK 字符串）\n",
    "- cut_all 参数：是否使用全模式，默认值为 False\n",
    "- HMM 参数：用来控制是否使用 HMM 模型，默认值为 True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "jieba.cut_for_search 和 jieba.lcut_for_search 接受 2 个参数：\n",
    "\n",
    "- 需要分词的字符串（unicode 或 UTF-8 字符串、GBK 字符串）\n",
    "- HMM 参数：用来控制是否使用 HMM 模型，默认值为 True\n",
    "\n",
    "尽量不要使用 GBK 字符串，可能无法预料地错误解码成 UTF-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)全模式和精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【全模式】：他/ 来到/ 上海/ 上海交通大学/ 交通/ 大学\n"
     ]
    }
   ],
   "source": [
    "# 全模式\n",
    "seg_list = jieba.cut(\"他来到上海交通大学\", cut_all=True)\n",
    "print(\"【全模式】：\" + \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【精确模式】：他/ 来到/ 上海交通大学\n"
     ]
    }
   ],
   "source": [
    "#精确模式\n",
    "seg_list = jieba.cut(\"他来到上海交通大学\", cut_all=False)\n",
    "print(\"【精确模式】：\" + \"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【返回列表】：['他', '来到', '上海', '上海交通大学', '交通', '大学']\n"
     ]
    }
   ],
   "source": [
    "# 返回列表\n",
    "seg_list = jieba.lcut(\"他来到上海交通大学\", cut_all=True)\n",
    "print(\"【返回列表】：{0}\".format(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 搜索引擎模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【搜索引擎模式】：他/ 毕业/ 于/ 上海/ 交通/ 大学/ 上海交通大学/ 机电/ 系/ ，/ 后来/ 在/ 一机部/ 上海/ 电器/ 科学/ 研究/ 研究所/ 工作\n"
     ]
    }
   ],
   "source": [
    "# 搜索引擎模式\n",
    "seg_list = jieba.cut_for_search(\"他毕业于上海交通大学机电系，后来在一机部上海电器科学研究所工作\")  \n",
    "print(\"【搜索引擎模式】：\" + \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【返回列表】：['他', '毕业', '于', '上海', '交通', '大学', '上海交通大学', '机电', '系', '，', '后来', '在', '一机部', '上海', '电器', '科学', '研究', '研究所', '工作']\n"
     ]
    }
   ],
   "source": [
    "# 返回列表\n",
    "seg_list = jieba.lcut_for_search(\"他毕业于上海交通大学机电系，后来在一机部上海电器科学研究所工作\")  \n",
    "print(\"【返回列表】：{0}\".format(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) HMM 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM 模型，即隐马尔可夫模型（Hidden Markov Model, HMM），是一种基于概率的统计分析模型，用来描述一个系统隐性状态的转移和隐性状态的表现概率。\n",
    "\n",
    "在 jieba 中，对于未登录到词库的词，使用了基于汉字成词能力的 HMM 模型和 Viterbi 算法，其大致原理是：\n",
    "    \n",
    "    采用四个隐含状态，分别表示为单字成词，词组的开头，词组的中间，词组的结尾。通过标注好的分词训练集，可以得到 HMM 的各个参数，然后使用 Viterbi 算法来解释测试集，得到分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【未启用 HMM】：他/ 来到/ 了/ 网易/ 杭/ 研/ 大厦\n"
     ]
    }
   ],
   "source": [
    "# 未启用 HMM\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\", HMM=False) #默认精确模式和启用 HMM\n",
    "print(\"【未启用 HMM】：\" + \"/ \".join(seg_list))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【识别新词】：他/ 来到/ 了/ 网易/ 杭研/ 大厦\n"
     ]
    }
   ],
   "source": [
    "# 识别新词\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") #默认精确模式和启用 HMM\n",
    "print(\"【识别新词】：\" + \"/ \".join(seg_list))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba 提供了两种关键词提取方法，分别基于 TF-IDF 算法和 TextRank 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 基于 TF-IDF 算法的关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)是一种统计方法，用以评估一个词语对于一个文件集或一个语料库中的一份文件的重要程度，其原理可概括为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一个词语在一篇文章中出现次数越多，同时在所有文档中出现次数越少，越能够代表该文章**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算公式：TF-IDF = TF * IDF，其中：\n",
    "\n",
    "- TF(term frequency, TF)：词频，某一个给定的词语在该文件中出现的次数，计算公式：\n",
    "\n",
    "        TFw = 在某一类中词条w出现的次数 / 该类中所有的词条数目\n",
    " \n",
    "- IDF(inverse document frequency, IDF)：逆文件频率，如果包含词条的文件越少，则说明词条具有很好的类别区分能力，计算公式\n",
    "\n",
    "        IDF = log( 语料库的文档总数/(包含词条w的文档数+1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 jieba.analyse.extract_tags 方法可以基于 TF-IDF 算法进行关键词提取，该方法共有 4 个参数：\n",
    "\n",
    "- sentence：为待提取的文本\n",
    "- topK：为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "- withWeight：是否一并返回关键词权重值，默认值为 False\n",
    "- allowPOS：仅包括指定词性的词，默认值为空\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =  \"\"\"\n",
    "此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，\n",
    "吉林欧亚置业注册资本由7000万元增加到5亿元。\n",
    "吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。\n",
    "目前在建吉林欧亚城市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 0.7300142700289363\n",
      "吉林 0.659038184373617\n",
      "置业 0.4887134522112766\n",
      "万元 0.3392722481859574\n",
      "增资 0.33582401985234045\n",
      "4.3 0.25435675538085106\n",
      "7000 0.25435675538085106\n",
      "2013 0.25435675538085106\n",
      "139.13 0.25435675538085106\n",
      "实现 0.19900979900382978\n",
      "综合体 0.19480309624702127\n",
      "经营范围 0.19389757253595744\n",
      "亿元 0.1914421623587234\n",
      "在建 0.17541884768425534\n",
      "全资 0.17180164988510638\n",
      "注册资本 0.1712441526\n",
      "百货 0.16734460041382979\n",
      "零售 0.1475057117057447\n",
      "子公司 0.14596045237787234\n",
      "营业 0.13920178509021275\n"
     ]
    }
   ],
   "source": [
    "for x, w in anls.extract_tags(s, topK=20, withWeight=True):\n",
    "    print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 **jieba.analyse.TFIDF(idf_path=None)** 可以新建 TFIDF 实例，其中 idf_path 为 IDF 频率文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 基于 TextRank 算法的关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank 是另一种关键词提取算法，基于大名鼎鼎的 PageRank。\n",
    "\n",
    "通过 **jieba.analyse.textrank** 方法可以使用基于 TextRank 算法的关键词提取，其与 'jieba.analyse.extract_tags' 有一样的参数，但前者默认过滤词性（allowPOS=('ns', 'n', 'vn', 'v')）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 1.0\n",
      "吉林 0.9835514435860004\n",
      "置业 0.6753582094217783\n",
      "实现 0.6107808193327324\n",
      "收入 0.4522580155631648\n",
      "增资 0.3848591236734226\n",
      "子公司 0.37270363916541444\n",
      "城市 0.36895795826072786\n",
      "商业 0.3673039455510563\n",
      "在建 0.33809801625898617\n",
      "零售 0.3186957815821605\n",
      "百货 0.31796821455453883\n",
      "全资 0.3168304245717673\n",
      "营业 0.3143229896315894\n",
      "综合体 0.3102041674755004\n",
      "注册资本 0.3024945728159306\n",
      "开发 0.2972332314434641\n",
      "有限公司 0.29482480259311533\n",
      "业务 0.2944444915502563\n",
      "经营范围 0.2806757163433113\n"
     ]
    }
   ],
   "source": [
    "for x, w in anls.textrank(s, withWeight=True):\n",
    "    print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 **jieba.analyse.TextRank()** 可以新建自定义 TextRank 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自定义语料库\n",
    "\n",
    "  关键词提取所使用逆向文件频率（IDF）文本语料库和停止词（Stop Words）文本语料库可以切换成自定义语料库的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-8a25cfaae59c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stop_words.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_idf_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"idf.txt.big\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jieba\\analyse\\tfidf.py\u001b[0m in \u001b[0;36mset_idf_path\u001b[1;34m(self, idf_path)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_abs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jieba: file does not exist: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew_abs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_new_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_abs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jieba\\analyse\\tfidf.py\u001b[0m in \u001b[0;36mset_new_path\u001b[1;34m(self, new_idf_path)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midf_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             self.median_idf = sorted(\n\u001b[1;32m---> 53\u001b[1;33m                 self.idf_freq.values())[len(self.idf_freq) // 2]\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "jieba.analyse.set_stop_words(\"stop_words.txt\")\n",
    "jieba.analyse.set_idf_path(\"idf.txt.big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 0.7458841454643479\n",
      "吉林 0.6733651014252174\n",
      "置业 0.49933765769413047\n",
      "万元 0.3466477318421739\n",
      "增资 0.3431245420230435\n",
      "4.3 0.25988625006304344\n",
      "7000 0.25988625006304344\n",
      "2013 0.25988625006304344\n",
      "139.13 0.25988625006304344\n",
      "实现 0.20333609898217392\n",
      "综合体 0.19903794616543477\n",
      "经营范围 0.19811273715630434\n",
      "亿元 0.19560394849695653\n",
      "在建 0.17923230089478262\n",
      "全资 0.17553646836086956\n",
      "注册资本 0.1749668515695652\n",
      "百货 0.1709825265097826\n",
      "零售 0.1507123576123913\n",
      "子公司 0.1491335056904348\n",
      "营业 0.14222791085304348\n"
     ]
    }
   ],
   "source": [
    "for x, w in anls.extract_tags(s, topK=20, withWeight=True):\n",
    "    print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 词性标注\n",
    "\n",
    "  jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。\n",
    "    \n",
    "    标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pseg.cut(\"他改变了中国\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他 r\n",
      "改变 v\n",
      "了 ul\n",
      "中国 ns\n"
     ]
    }
   ],
   "source": [
    "for word, flag in words:\n",
    "    print(\"{0} {1}\".format(word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 并行分词\n",
    "\n",
    "将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升。用法：\n",
    "\n",
    "- jieba.enable_parallel(4)：开启并行分词模式，参数为并行进程数\n",
    "- jieba.disable_parallel() ：关闭并行分词模式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 返回词语在原文的起止位置\n",
    "\n",
    "使用 jieba.tokenize 方法可以返回词语在原文的起止位置。\n",
    "\n",
    "注意：输入参数只接受 unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【普通模式】\n",
      "word: 上海 \t\t start: 0 \t\t end: 2\n",
      "word: 益民 \t\t start: 2 \t\t end: 4\n",
      "word: 食品 \t\t start: 4 \t\t end: 6\n",
      "word: 一厂 \t\t start: 6 \t\t end: 8\n",
      "word: 有限公司 \t\t start: 8 \t\t end: 12\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'上海益民食品一厂有限公司')\n",
    "print(\"【普通模式】\")\n",
    "for tk in result:\n",
    "    print(\"word: {0} \\t\\t start: {1} \\t\\t end: {2}\".format(tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型之向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在词袋模型的统计词频这一步，我们会得到该文本中所有词的词频，有了词频，我们就可以用词向量表示这个文本。这里我们举一个例子，例子直接用scikit-learn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 7)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 15)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    " \n",
    "vectorizer=CountVectorizer()\n",
    "corpus=[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    " \n",
    "print (vectorizer.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出4个文本的词频已经统计出，在输出中，左边的括号中的第一个数字是文本的序号，第2个数字是词的序号，注意词的序号是基于所有的文档的。第三个数字就是我们的词频。\n",
    "\n",
    "我们可以进一步看看每个文本的词向量特征和各个特征代表的词，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
      " [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n",
      "['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write']\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(corpus).toarray())\n",
    "print (vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们一共有19个词，所以4个文本都是19维的特征向量。而每一维的向量依次对应了下面的19个词。另外由于词\"I\"在英文中是停用词，不参加词频的统计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF的sklearn实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn库中有两种TF-IDF实现方式：\n",
    "\n",
    "- 1.CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理\n",
    "\n",
    "- 2.用TfidfVectorizer完成向量化与TF-IDF预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t0.4424621378947393\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 4)\t0.4424621378947393\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (2, 12)\t0.5\n",
      "  (2, 7)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 15)\t0.2811316284405006\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 10)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 5)\t0.2811316284405006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    " \n",
    "corpus=[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    " \n",
    "vectorizer=CountVectorizer()\n",
    " \n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))  \n",
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.4424621378947393\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (0, 16)\t0.4424621378947393\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (2, 7)\t0.5\n",
      "  (2, 12)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (3, 15)\t0.2811316284405006\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 5)\t0.2811316284405006\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 10)\t0.3565798233381452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "tfidf2 = TfidfVectorizer()\n",
    "re = tfidf2.fit_transform(corpus)\n",
    "print (re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.34884223, 0.44246214,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.69768446, 0.44246214, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.4533864 , 0.35745504, 0.        ,\n",
       "         0.35745504, 0.35745504, 0.        , 0.        , 0.4533864 ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.4533864 ,\n",
       "         0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.5       , 0.5       , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.28113163, 0.28113163, 0.        , 0.35657982, 0.        ,\n",
       "         0.35657982, 0.35657982, 0.        , 0.35657982, 0.        ,\n",
       "         0.28113163, 0.        , 0.35657982, 0.35657982]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 互信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其衡量的是两个随机变量之间的相关性，即一个随机变量中包含的关于另一个随机变量的信息量。所谓的随机变量，即随机试验结\n",
    "\n",
    "果的量的表示，可以简单理解为按照一个概率分布进行取值的变量，比如随机抽查的一个人的身高就是一个随机变量。\n",
    "\n",
    "可以看出，互信息其实就是对X和Y的所有可能的取值情况的点互信息PMI的加权和。因此，点互信息这个名字还是很形象的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "互信息的sklearn实现(label和x之间的互信息值)\n",
    "\n",
    "    from sklearn import metrics as mr\n",
    "    mr.mutual_info_score(label,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在利用以上语句对生成的特征矩阵进行特征筛选："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
      " [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n",
      "0.09232984929764276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics as mr\n",
    " \n",
    "corpus=[\"I come to China to travel\",\n",
    "    \"This is a car polupar in China\",\n",
    "    \"I love tea and Apple \",\n",
    "    \"The work is to write some papers in science\"]\n",
    " \n",
    "vectorizer=CountVectorizer()\n",
    " \n",
    "transformer = TfidfTransformer()\n",
    "#tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "a = vectorizer.fit_transform(corpus).toarray()\n",
    "print(a)\n",
    "print(mr.mutual_info_score(a[1],a[2]))\n",
    "#print (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t0.4424621378947393\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 4)\t0.4424621378947393\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (2, 12)\t0.5\n",
      "  (2, 7)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 15)\t0.2811316284405006\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 10)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 5)\t0.2811316284405006\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
