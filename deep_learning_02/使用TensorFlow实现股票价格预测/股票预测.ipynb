{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用Google Finance API抓取了每分钟的标准普尔500指数。除了标准普尔500指数以外，还收集了其对应的500家公司的股价。在得到了这些数据之后，基于标准普尔指数观察的500家公司的股价，用深度学习模型来预测标准普尔500指数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：本文只是基于TensorFlow的一个实战教程。真正预测股价是非常具有挑战性的，尤其在分钟级这样频率较高的预测中，要考虑的因素的量是庞大的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抓取到的股票数据从爬虫服务器上导出为CSV格式的文件。该数据集包含了从2017年四月到八月共计n=41266分钟的标准普尔500指数以及500家公司的股价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "df = pd.read_csv('data_stocks.csv')\n",
    "# 移除日期列\n",
    "df = df.drop(['DATE'], 1)\n",
    "# 数据集的维度\n",
    "n = df.shape[0]\n",
    "p = df.shape[1]\n",
    "# 将数据集转化为numpy数组\n",
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41266 entries, 0 to 41265\n",
      "Columns: 501 entries, SP500 to NYSE.ZTS\n",
      "dtypes: float64(501)\n",
      "memory usage: 157.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SP500', 'NASDAQ.AAL', 'NASDAQ.AAPL', 'NASDAQ.ADBE', 'NASDAQ.ADI',\n",
       "       'NASDAQ.ADP', 'NASDAQ.ADSK', 'NASDAQ.AKAM', 'NASDAQ.ALXN',\n",
       "       'NASDAQ.AMAT',\n",
       "       ...\n",
       "       'NYSE.WYN', 'NYSE.XEC', 'NYSE.XEL', 'NYSE.XL', 'NYSE.XOM', 'NYSE.XRX',\n",
       "       'NYSE.XYL', 'NYSE.YUM', 'NYSE.ZBH', 'NYSE.ZTS'],\n",
       "      dtype='object', length=501)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过pyplot.plot('SP500')来快速查看标准普尔500指数的时间序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准普尔500指数的时间序列图\n",
    "    \n",
    "注意：这里展示的是标普500指数的领先(lead)，也就是说其值是原始值在时间轴上后移一分钟得到的。因为我们要预测的是下一分钟的指数而不是当前的指数，所以这一操作是必不可少的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练集和测试集数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集被分成训练集和测试集。训练数据为总数据集的80％。数据不进行打乱，而是按顺序切片。训练数据可以从2017年4月选取到2017年7月底，测试数据则选取到2017年8月底为止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train_start = 0\n",
    "train_end = int(np.floor(0.8*n))\n",
    "test_start = train_end + 1\n",
    "test_end = n\n",
    "data_train = data[np.arange(train_start, train_end), :]\n",
    "data_test = data[np.arange(test_start, test_end), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "时间序列的交叉验证方法有很多，像有无refitting或其他像time series bootstrap resampling的精细概念的滚动预测（rolling forecasts）。后者（time series bootstrap resampling）中的重复样本是考虑时间序列的周期性分解的结果，这是为了使模拟采样同样具有周期性的特征而不是单单复制采样值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数的神经网络都受益于输入值的缩放（有时也有输出值）。为什么呢？\n",
    "\n",
    "因为大多数神经网络的激励函数都是定义在0, 1区间或-1, 1区间，像sigmoid函数和tanh函数一样。\n",
    "\n",
    "虽然如今线性整流单元已经被广泛引用于无界的激活值问题中，但是我们还是选择将输入输出值做统一的缩放。\n",
    "\n",
    "缩放操作可以通过sklearn中的MinMaxScaler轻松实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据缩放\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "\n",
    "scaler_test = MinMaxScaler()\n",
    "scaler_test.fit(data_test)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8253, 500) (8253, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data_train[:, 1:]\n",
    "Y = data_train[:, 0]\n",
    "Y.shape = (8253,1)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "X_test = data_test[:, 1:]\n",
    "Y_test = data_test[:, 0]\n",
    "Y_test.shape = (8253,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "备注：应当仔细考虑好什么数据要在什么时候被缩放。一个常见的错误是在训练集和测试集划分前进行特征缩放。为什么这样做是错误的呢？\n",
    "\n",
    "因为缩放的计算需要调用数据的统计值（像数据的最大最小值）。当你在真实生活中进行预测时你并没有来自未来的观测信息，所以相应地，训练数据特征缩放所用的统计值应当来源于训练集，测试集也一样。否则，在预测时使用了包含未来信息往往会导致性能指标向好的方向偏移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络流程\n",
    "\n",
    "    1数据集获取（有监督数据整理）\n",
    "    2神经网络参数确定，有多少层，多少个节点，激活函数是什么，损失函数是什么\n",
    "    3数据预处理，标准化，中心化，特征压缩，异常值处理\n",
    "    4初始化网络权重\n",
    "    5网络训练\n",
    "            5.1正向传播\n",
    "            5.2计算loss\n",
    "            5.3计算反向梯度\n",
    "            5.4更新梯度\n",
    "            5.5重新正向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 占位符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的过程都从占位符开始。为了拟合模型，我们需要定义两个占位符：X包含模型输入（在T = t时刻500个成员公司的股价），Y为模型输出（T = t + 1时刻的标普指数）。\n",
    "\n",
    "占位符的shape分别为[None, n_stocks]和[None]，意味着输入为二维矩阵，输出为一维向量。设计出恰当的神经网络的必要条件之一就是清楚神经网络需要的输入和输出维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 占位符\n",
    "n_stack = 500\n",
    "x = tf.placeholder(tf.float32, [None, n_stack])\n",
    "y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None值代表着我们当前不知道每个批次中流经神经网络的观测值数量，所以为了保持该量的弹性，我们用None来填充。\n",
    "\n",
    "稍后我们将定义控制每个批次中观测样本数量的变量batch_size。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了占位符，TensorFlow中的另一个基本概念是变量。\n",
    "\n",
    "占位符在图中用来存储输入数据和输出数据，变量在图的执行过程中可以变化，是一个弹性的容器。\n",
    "\n",
    "为了在训练中调整权重和偏置，它们被定义为变量。变量需要在训练开始前进行初始化。变量的初始化稍后我们会单独讲解。\n",
    "\n",
    "我们的模型包含四个层。第一层有1024个神经元，比输入变量的两倍还要多一点。紧接在后面的隐藏层是前面一层的一半，即后面层的神经元个数分别为512,256和128。每层中神经元数量的减少也意味着信息量的压缩。当然还有其他的神经网络结构，但是不在本文的讨论范围当中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??tf.random_normal"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "初始化器\n",
    "\n",
    "初始化器用于在训练前初始化网络的权重。由于神经网络是利用数值方法进行训练，所以优化问题的起始点是能否找到问题的最优解（或次优解）的关键因素之一。TensorFlow中内置了多种优化器，每个优化器使用了不同的初始化方法。\n",
    "\n",
    "sigma = 1\n",
    "weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\",  distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权重和偏置项初始化\n",
    "def weight_init(shape):\n",
    "    return tf.random_normal(shape)\n",
    "\n",
    "def bias_init(shape):\n",
    "    return tf.random_normal(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型结构参数\n",
    "n1 = 1024\n",
    "n2 = 512\n",
    "n3 = 256\n",
    "n4 = 128\n",
    "n_target = 1\n",
    "\n",
    "# 第一层 : 隐藏层权重和偏置变量\n",
    "w1 = tf.Variable(weight_init([n_stack, n1]))\n",
    "b1 = tf.Variable(bias_init([n1]))\n",
    "\n",
    "# 第二层 : 隐藏层权重和偏置变量\n",
    "w2 = tf.Variable(weight_init([n1, n2]))\n",
    "b2 = tf.Variable(bias_init([n2]))\n",
    "\n",
    "# 第三层: 隐藏层权重和偏置变量\n",
    "w3 = tf.Variable(weight_init([n2, n3]))\n",
    "b3 = tf.Variable(bias_init([n3]))\n",
    "\n",
    "# 第四层: 隐藏层权重和偏置变量\n",
    "w4 = tf.Variable(weight_init([n3, n4]))\n",
    "b4 = tf.Variable(bias_init([n4]))\n",
    "\n",
    "# 输出层: 输出权重和偏置变量\n",
    "w_out = tf.Variable(weight_init([n4, n_target]))\n",
    "b_out = tf.Variable(bias_init([n_target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "清楚输入层、隐藏层和输出层的变量对应的维度是非常重要的。\n",
    "\n",
    "在多层感知机的经验法则中（MLPs，本文就是按照该准则设计的网络），前一层权重的维度数组中的第二个元素与当前层中权重维度数组的第一个元素数值相等。听起来可能有些复杂，但是为了使当前层的输入作为输入传入下一层，这样的法则是必要的。偏置的维度等于当前层权重维度数组中的第二个元素，对应当前层中神经元的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设计网络架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义了所需的权重和偏置变量之后，网络的拓扑结构即网络的架构需要被确定下来。在TensorFlow中，即需要将占位符（数据）和变量（权重和偏置）整合入矩阵乘法的序列当中。\n",
    "\n",
    "除此之外，神经网络中是经过了激活函数的转换的。激活函数是神经网络架构中非常的元素之一，在非线性系统中尤其如此。目前已经有很多中可供使用的激活函数，本文中的模型选用了最常用的整流线性单元（ReLU）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层\n",
    "L1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "L2 = tf.nn.relu(tf.matmul(L1, w2) + b2)\n",
    "L3 = tf.nn.relu(tf.matmul(L2, w3) + b3)\n",
    "L4 = tf.nn.relu(tf.matmul(L3, w4) + b4)\n",
    "\n",
    "# 输出层 (必须经过转置)\n",
    "out = tf.transpose(tf.matmul(L4, w_out) + b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图形说明了网络架构。模型一共包含了三个主要的组件：输入层，隐藏层和输出层。图示的结构被称为前馈网络，前馈意味着从左侧输入的数据将径自向右传播。与之相对的网络结构如recurrent neural networks（RNN）允许数据流在网络结构中反向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的损失函数可以根据网络的预测值和训练集中的实际观测值来生成度量偏差程度的指标。\n",
    "\n",
    "在回归问题当中，最常用的损失函数为均方误差（MSE）。均方误差计算的就是预测值和目标值的误差平方值的平均值。基本上任何可微函数都可以用于计算预测值和目标值之间的偏差程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数   tf.square(out-Y)\n",
    "loss = tf.reduce_mean(tf.square(out - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器负责训练过程中调整网络的权重和偏置的关键操作。这些操作中包含着梯度运算，梯度方向对应的就是训练过程中最小化网络损失函数的方向。稳定而又高效的优化器是神经网络中深入研究的课题之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "train = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用Adam优化器，目前它是深度学习中默认的优化器。Adam的全称为Adaptive Moment Estimation，可以视为其他两个优化器AdaGrad和RMSProp的结合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义了网络的占位符，变量，初始化器，损失函数和优化器之后，模型需要进入正式的训练过程。通常我们使用minibatch的方式进行训练（小的batch size）。在这种训练方式中，我们从训练集中随机抽取n = sample_size的数据样本送入网络进行训练。训练集被划分为n / batch_size个批次并按顺序送入网络。这时占位符X和Y参与了这一过程，它们分别存储输入值和目标值并作为输入和目标送入网络。\n",
    "\n",
    "样本数据X将在网络中传播直至输出层。到达输出层后，TensorFlow将把模型的当前预测值与当前批次的实际观测值Y进行比较。随后，TensorFlow将根据选择的学习方案对网络参数进行优化更新。权重和偏置更新完毕后，下一批采样数据将再次送入网络并重复这一过程。这一过程将一直持续至所有批次的数据都已经送入网络。所有的批次构成的一个完整训练过程被称为一个epoch。\n",
    "\n",
    "当达到训练批次数或者用户指定的标准之后，网络的训练停止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "子目录或文件 img 已经存在。\n"
     ]
    }
   ],
   "source": [
    "!mkdir img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n",
      "6031023.5\n"
     ]
    }
   ],
   "source": [
    "# 定义会话\n",
    "sess = tf.Session()\n",
    "# 运行初始化器\n",
    "sess.run(tf.global_variables_initializer())\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "for i in range(epochs):\n",
    "    suffle = np.random.permutation(np.arange(len(Y)))\n",
    "    xtrain = X[suffle]\n",
    "    ytrain = Y[suffle]\n",
    "    # minbatch 训练\n",
    "    for j in range(len(Y)):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        sess.run(train, feed_dict={x:xtrain[start:end], y:ytrain[start:end]})\n",
    "    \n",
    "    final = sess.run(loss, feed_dict={x:X_test, y:Y_test})\n",
    "    print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里再给出一些可以进一步提升结果的方法：规划网络层数和神经元个数，选择不同的初始化和激活方案，引入神经元的dropout层，early stopping等等。除此之外，换用其他类型的深度学习模型，比方说RNN也许可以在任务上达到更优的性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
